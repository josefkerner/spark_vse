{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"use_cases\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PTB model in telco\n",
    "For this project we developed near complete automated pipeline. Here is one example of custom transformer, it's called Sampling transformer and is used to oversampling/undersampling data for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer, Estimator, Pipeline\n",
    "from pyspark.sql.functions import column as col\n",
    "from pyspark.sql.functions import explode, array, lit\n",
    "\n",
    "class SamplingTransformer(Transformer):\n",
    "    \n",
    "    def __init__(self, labelCol = 'label', split = [0.7, 0.3], oversampling = 10, undersampling = 1, minority = 1):\n",
    "        \"\"\" This is a custom oversampling transformer used to oversample positive or negative observation \n",
    "        \n",
    "        Attributes:\n",
    "            labelCol (str): Columns with target variable. Defaults to label\n",
    "            split (list(float)): Split to be used for the train/test ratio. Defaults to [0.7, 0.3].\n",
    "            oversampling (int): Factor of the oversampling. Defaults to 10.\n",
    "            undersampling (int): Factor of the undersampling. Defaults to 1.\n",
    "            minority (int): Which of the observation is the minority to be sampled. Defaults to 1.\n",
    "        \"\"\" \n",
    "        \n",
    "        super(SamplingTransformer, self).__init__()\n",
    "        \n",
    "        # Assign the constructor varibles\n",
    "        self.labelCol = labelCol\n",
    "        self.split = split\n",
    "        self.oversampling = oversampling\n",
    "        \n",
    "        # Check if undersampling is smaller than one\n",
    "        if undersampling < 1:\n",
    "          print(\"undersampling must be greater than 1\")\n",
    "          undersampling = 1\n",
    "          \n",
    "        self.undersampling = undersampling\n",
    "        self.minority = minority\n",
    "\n",
    "    def sample(self, dataset):\n",
    "        \"\"\" This function takes input dataset and does the sampling based upon the inpur parameters.\"\"\"\n",
    "        \n",
    "        # Create list in range of the sample factor\n",
    "        samples = range(0, self.oversampling)\n",
    "        \n",
    "        # Create a new column 'dummy' with array in range of samples and explode it. This is the fastest way to oversample in pySpark.\n",
    "        if self.minority == 1:\n",
    "            \n",
    "          # Split the positive observations\n",
    "          train_data_positive,test_data_positive = dataset.filter(col(self.labelCol) == 1).randomSplit([0.7,0.3])\n",
    "        \n",
    "          # Split the negative observations\n",
    "          train_data_negative,test_data_negative = dataset.filter(col(self.labelCol) == 0).randomSplit([(0.7/self.undersampling), 0.3])\n",
    "          \n",
    "          train_data_positive = train_data_positive.withColumn(\"dummy\", explode(array([lit(x) for x in samples]))).drop(\"dummy\")\n",
    "        \n",
    "        else:\n",
    "          \n",
    "          # Split the positive observations\n",
    "          train_data_positive,test_data_positive = dataset.filter(col(self.labelCol) == 1).randomSplit([(0.7/self.undersampling),0.3])\n",
    "        \n",
    "          # Split the negative observations\n",
    "          train_data_negative,test_data_negative = dataset.filter(col(self.labelCol) == 0).randomSplit([0.7, 0.3])\n",
    "          \n",
    "          train_data_negative = train_data_negative.withColumn(\"dummy\", explode(array([lit(x) for x in samples]))).drop(\"dummy\")\n",
    "        \n",
    "        # Union positive and negative train/test dataframes\n",
    "        train_data = train_data_negative.union(train_data_positive)\n",
    "        test_data = test_data_negative.union(test_data_positive)\n",
    "        \n",
    "        return (train_data, test_data)\n",
    "        \n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        return self.sample(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = spark.read.parquet('/appl/wsp_data_science_workshop/cleanData_churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how much of each target variable we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 1869|\n",
      "|    0| 5174|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to oversample the label = 1 by factor of 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = SamplingTransformer(oversampling=3)\n",
    "\n",
    "train_data, test_data = st.transform(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 3948|\n",
      "|    0| 3634|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|  553|\n",
      "|    0| 1540|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recommendation engine - usage of config\n",
    "Engine need to run for different segments. Rather than duplicating and making small changes to code, we use Config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config/config.ini']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from configparser import ConfigParser, ExtendedInterpolation\n",
    "\n",
    "config = ConfigParser(interpolation=ExtendedInterpolation())\n",
    "\n",
    "config.read('config/config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'master_database.output'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['MicroClustering']['output_table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'latest'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['MasterParameters']['date_valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
